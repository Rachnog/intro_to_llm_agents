{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install the following libraries in a fresh .venv environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain\n",
    "# ! pip install -U langchain-community tavily-python\n",
    "# ! pip install --upgrade --quiet langchain-openai tavily-python\n",
    "# ! pip install beautifulsoup4\n",
    "# ! pip install faiss-cpu\n",
    "# ! pip install langchainhub\n",
    "# ! pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, ensure that you have registered in the following services and got the API key stored in the configuration file:\n",
    "\n",
    "- Tavily - [Langchain search tool](https://app.tavily.com/home)\n",
    "- Langsmith - [LLM tracing tool](https://smith.langchain.com/)\n",
    "- Weights&Biases - [LLM tracing tool](https://wandb.ai/site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "config = yaml.safe_load(open(\"myconfig.yml\"))\n",
    "os.environ[\"OPENAI_API_KEY\"] = config[\"OPEN_AI_API_KEY\"]\n",
    "os.environ[\"TAVILY_API_KEY\"] = config[\"TAVILY_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = config[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = str(config[\"LANGCHAIN_TRACING_V2\"]).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning techniques\n",
    "\n",
    "![Screenshot 2024-02-21 at 16.17.22.png](<images/Screenshot 2024-02-21 at 16.17.22.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree of Thoughts\n",
    "\n",
    "https://medium.com/@astropomeai/implementing-the-tree-of-thoughts-in-langchains-chain-f2ebc5864fac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "template =\"\"\"\n",
    "Step1 :\n",
    " \n",
    "I have a problem related to {input}. Could you brainstorm three distinct solutions? Please consider a variety of factors such as {perfect_factors}\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\",\"perfect_factors\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain1 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"solutions\"\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 2:\n",
    "\n",
    "For each of the three proposed solutions, evaluate their potential. Consider their pros and cons, initial effort needed, implementation difficulty, potential challenges, and the expected outcomes. Assign a probability of success and a confidence level to each option based on these factors\n",
    "\n",
    "{solutions}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"solutions\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"review\"\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 3:\n",
    "\n",
    "For each solution, deepen the thought process. Generate potential scenarios, strategies for implementation, any necessary partnerships or resources, and how potential obstacles might be overcome. Also, consider any potential unexpected outcomes and how they might be handled.\n",
    "\n",
    "{review}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"review\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain3 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"deepen_thought_process\"\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 4:\n",
    "\n",
    "Based on the evaluations and scenarios, rank the solutions in order of promise. Provide a justification for each ranking and offer any final thoughts or considerations for each solution\n",
    "{deepen_thought_process}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"deepen_thought_process\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain4 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"ranked_solutions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
