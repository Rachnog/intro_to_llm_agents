{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "config = yaml.safe_load(open(\"myconfig.yml\"))\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = config[\"OPEN_AI_API_KEY\"]\n",
    "os.environ[\"TAVILY_API_KEY\"] = config[\"TAVILY_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = config[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_HUB_API_KEY\"] = config[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = str(config[\"LANGCHAIN_TRACING_V2\"]).lower()\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = config[\"LANGCHAIN_ENDPOINT\"]\n",
    "os.environ[\"LANGCHAIN_HUB_API_URL\"] = config[\"LANGCHAIN_HUB_API_URL\"]\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = str(config[\"LANGCHAIN_WANDB_TRACING\"]).lower()\n",
    "os.environ[\"WANDB_PROJECT\"] = config[\"WANDB_PROJECT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning\n",
    "\n",
    "![images/Screenshot 2024-02-21 at 16.17.22.png](<images/Screenshot 2024-02-21 at 16.17.22.png>)\n",
    "\n",
    "You might have come across various techniques aimed at improving the performance of large language models, such as offering tips or even jokingly threatening them. One popular technique is called \"chain of thought,\" where the model is asked to think step by step, enabling self-correction. This approach has evolved into more advanced versions like \"chain of thought with self-consistency\" and the generalized \"tree of thoughts,\" where multiple thoughts are created, re-evaluated, and consolidated to provide an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree of Thoughts\n",
    "\n",
    "The [@astropomeai tutorial](https://medium.com/@astropomeai/implementing-the-tree-of-thoughts-in-langchains-chain-f2ebc5864fac) on Tree of Thoughts is used as basis of this exercise but expanded with LLMOps tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "cot_step1 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step1\")\n",
    "cot_step2 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step2\")\n",
    "cot_step3 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step3\")\n",
    "cot_step4 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oleksandrhonchar/Documents/GitHub/intro_to_llm_agents/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "chain1 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model),\n",
    "    prompt=cot_step1,\n",
    "    output_key=\"solutions\"\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model),\n",
    "    prompt=cot_step2,\n",
    "    output_key=\"review\"\n",
    ")\n",
    "\n",
    "chain3 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model),\n",
    "    prompt=cot_step3,\n",
    "    output_key=\"deepen_thought_process\"\n",
    ")\n",
    "\n",
    "chain4 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model),\n",
    "    prompt=cot_step4,\n",
    "    output_key=\"ranked_solutions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oleksandrhonchar/Documents/GitHub/intro_to_llm_agents/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Streaming LangChain activity to W&B at https://wandb.ai/rachnogstyle/nlw_jan24/runs/mb4q3lgw\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: `WandbTracer` is currently in beta.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `langchain`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WARNING: Failed to serialize model: 'int' object is not subscriptable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'human colonization of Mars', 'perfect_factors': 'The distance between Earth and Mars is very large, making regular resupply difficult', 'ranked_solutions': 'This solution ranks highest in promise due to its potential to address multiple challenges faced by Martian colonies, such as food production, shelter construction, and resource management. By creating self-sufficient habitats, colonists can reduce their reliance on Earth for supplies and increase their chances of long-term survival on Mars. The strategies for implementation, such as collaboration with experts and gradual scaling up of technology, demonstrate a thoughtful approach to overcoming potential obstacles. Additionally, the possibility of unexpected outcomes, such as the discovery of new technologies, adds to the promise of this solution. However, it is important to consider the need for extensive research and testing to ensure the success of this solution.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"input\", \"perfect_factors\"],\n",
    "    output_variables=[\"ranked_solutions\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\n",
    "        overall_chain(\n",
    "            {\n",
    "                \"input\": \"human colonization of Mars\",\n",
    "                \"perfect_factors\": \"The distance between Earth and Mars is very large, making regular resupply difficult\"\n",
    "            }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct prompt overview\n",
    "\n",
    "Let's review [ReAct](https://python.langchain.com/docs/modules/agents/agent_types/react) prompt as it's defined in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-ask with search\n",
    "\n",
    "Let's review [self-ask with search](https://python.langchain.com/docs/modules/agents/agent_types/self_ask_with_search) as it's defined in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Who lived longer, Muhammad Ali or Alan Turing?\\nAre follow up questions needed here: Yes.\\nFollow up: How old was Muhammad Ali when he died?\\nIntermediate answer: Muhammad Ali was 74 years old when he died.\\nFollow up: How old was Alan Turing when he died?\\nIntermediate answer: Alan Turing was 41 years old when he died.\\nSo the final answer is: Muhammad Ali\\n\\nQuestion: When was the founder of craigslist born?\\nAre follow up questions needed here: Yes.\\nFollow up: Who was the founder of craigslist?\\nIntermediate answer: Craigslist was founded by Craig Newmark.\\nFollow up: When was Craig Newmark born?\\nIntermediate answer: Craig Newmark was born on December 6, 1952.\\nSo the final answer is: December 6, 1952\\n\\nQuestion: Who was the maternal grandfather of George Washington?\\nAre follow up questions needed here: Yes.\\nFollow up: Who was the mother of George Washington?\\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\\nFollow up: Who was the father of Mary Ball Washington?\\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\\nSo the final answer is: Joseph Ball\\n\\nQuestion: Are both the directors of Jaws and Casino Royale from the same country?\\nAre follow up questions needed here: Yes.\\nFollow up: Who is the director of Jaws?\\nIntermediate answer: The director of Jaws is Steven Spielberg.\\nFollow up: Where is Steven Spielberg from?\\nIntermediate answer: The United States.\\nFollow up: Who is the director of Casino Royale?\\nIntermediate answer: The director of Casino Royale is Martin Campbell.\\nFollow up: Where is Martin Campbell from?\\nIntermediate answer: New Zealand.\\nSo the final answer is: No\\n\\nQuestion: {input}\\nAre followup questions needed here:{agent_scratchpad}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/self-ask-with-search\")\n",
    "prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approaches to read next:\n",
    "\n",
    "**Reflexion** ([Shinn & Labash 2023](https://arxiv.org/abs/2303.11366)) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills.\n",
    "\n",
    "**Chain of Hindsight** (CoH; [Liu et al. 2023](https://arxiv.org/abs/2302.02676)) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
