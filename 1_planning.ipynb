{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain\n",
    "# ! pip install -U langchain-community tavily-python\n",
    "# ! pip install --upgrade --quiet langchain-openai tavily-python\n",
    "# ! pip install beautifulsoup4\n",
    "# ! pip install faiss-cpu\n",
    "# ! pip install langchainhub\n",
    "# ! pip install -U wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, ensure that you have registered in the following services and got the API key stored in the configuration file:\n",
    "\n",
    "- Tavily - [Langchain search tool](https://app.tavily.com/home)\n",
    "- Langsmith - [LLM tracing tool](https://smith.langchain.com/)\n",
    "- Weights&Biases - [MLOps / LLM tracing tool](https://wandb.ai/site)\n",
    "\n",
    "LLMOps resources used for this tutorial: [LangSmith](https://github.com/langchain-ai/langsmith-cookbook/blob/main/hub-examples/retrieval-qa-chain-versioned/prompt-versioning.ipynb) and [Weights&Biases](https://docs.wandb.ai/guides/prompts/quickstart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "config = yaml.safe_load(open(\"myconfig.yml\"))\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = config[\"OPEN_AI_API_KEY\"]\n",
    "os.environ[\"TAVILY_API_KEY\"] = config[\"TAVILY_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = config[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_HUB_API_KEY\"] = config[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = str(config[\"LANGCHAIN_TRACING_V2\"]).lower()\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = config[\"LANGCHAIN_ENDPOINT\"]\n",
    "os.environ[\"LANGCHAIN_HUB_API_URL\"] = config[\"LANGCHAIN_HUB_API_URL\"]\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = str(config[\"LANGCHAIN_WANDB_TRACING\"]).lower()\n",
    "os.environ[\"WANDB_PROJECT\"] = config[\"WANDB_PROJECT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning techniques\n",
    "\n",
    "![Screenshot 2024-02-21 at 16.17.22.png](<images/Screenshot 2024-02-21 at 16.17.22.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree of Thoughts\n",
    "\n",
    "The [@astropomeai tutorial](https://medium.com/@astropomeai/implementing-the-tree-of-thoughts-in-langchains-chain-f2ebc5864fac) on Tree of Thoughts is used as basis of this exercise but expanded with LLMOps tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "cot_step1 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step1\")\n",
    "cot_step2 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step2\")\n",
    "cot_step3 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step3\")\n",
    "cot_step4 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oleksandrhonchar/Documents/GitHub/intro_to_llm_agents/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "chain1 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model),\n",
    "    prompt=cot_step1,\n",
    "    output_key=\"solutions\"\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model),\n",
    "    prompt=cot_step2,\n",
    "    output_key=\"review\"\n",
    ")\n",
    "\n",
    "chain3 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model),\n",
    "    prompt=cot_step3,\n",
    "    output_key=\"deepen_thought_process\"\n",
    ")\n",
    "\n",
    "chain4 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model),\n",
    "    prompt=cot_step4,\n",
    "    output_key=\"ranked_solutions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oleksandrhonchar/Documents/GitHub/intro_to_llm_agents/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Streaming LangChain activity to W&B at https://wandb.ai/rachnogstyle/nlw_jan24/runs/o2bahzqu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: `WandbTracer` is currently in beta.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `langchain`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'human colonization of Mars', 'perfect_factors': 'The distance between Earth and Mars is very large, making regular resupply difficult', 'ranked_solutions': \"Developing sustainable and self-sufficient habitats on Mars ranks highest in promise due to its comprehensive approach to addressing the challenges of living on the red planet. By focusing on advanced technologies for food and oxygen production, as well as resilient habitats, this solution offers a long-term and sustainable solution for Martian colonization. The potential scenarios of partnerships with private space companies or government agencies provide a solid foundation for collaboration and resource pooling. Strategies for implementation, such as feasibility studies and testing prototypes, demonstrate a practical and systematic approach to achieving the goal. Potential obstacles, such as technical failures, are addressed through redundancy in systems and regular maintenance protocols, showing a proactive approach to risk management. The integration of unexpected outcomes, such as the discovery of new technologies, further enhances the potential success of this solution.\\n\\nEstablishing a system of regular cargo missions to Mars ranks second in promise, as it addresses the critical need for a reliable supply chain to support Martian colonies. The potential scenarios of collaboration between space agencies, private companies, and international partners demonstrate a realistic and achievable approach to coordinating missions. Strategies for implementation, such as developing reusable spacecraft and conducting regular supply chain audits, show a focus on efficiency and cost-effectiveness. Potential obstacles, such as budget constraints, are addressed through partnerships with commercial space companies or government funding initiatives, indicating a proactive approach to overcoming challenges. The potential for unexpected outcomes, such as the discovery of new resources, further enhances the potential success of this solution.\\n\\nUtilizing resources available on Mars for local manufacturing ranks third in promise, as it offers a practical approach to leveraging the planet's resources for economic growth. The potential scenarios of partnerships with mining companies and research institutions provide a solid foundation for resource extraction and manufacturing. Strategies for implementation, such as developing automated processes and establishing quality control measures, demonstrate a systematic approach to building a local economy. Potential obstacles, such as regulatory challenges, are addressed through international agreements and sustainable practices, showing a commitment to responsible resource management. The potential for unexpected outcomes, such as the discovery of rare minerals, further enhances the potential success of this solution.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"input\", \"perfect_factors\"],\n",
    "    output_variables=[\"ranked_solutions\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\n",
    "        overall_chain(\n",
    "            {\n",
    "                \"input\": \"human colonization of Mars\",\n",
    "                \"perfect_factors\": \"The distance between Earth and Mars is very large, making regular resupply difficult\"\n",
    "            }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct prompt overview\n",
    "\n",
    "Let's review [ReAct](https://python.langchain.com/docs/modules/agents/agent_types/react) prompt as it's defined in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-ask with search\n",
    "\n",
    "Let's review [self-ask with search](https://python.langchain.com/docs/modules/agents/agent_types/self_ask_with_search) as it's defined in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/self-ask-with-search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the founder of craigslist?\n",
      "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "Follow up: When was Craig Newmark born?\n",
      "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "So the final answer is: December 6, 1952\n",
      "\n",
      "Question: Who was the maternal grandfather of George Washington?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who is the director of Jaws?\n",
      "Intermediate answer: The director of Jaws is Steven Spielberg.\n",
      "Follow up: Where is Steven Spielberg from?\n",
      "Intermediate answer: The United States.\n",
      "Follow up: Who is the director of Casino Royale?\n",
      "Intermediate answer: The director of Casino Royale is Martin Campbell.\n",
      "Follow up: Where is Martin Campbell from?\n",
      "Intermediate answer: New Zealand.\n",
      "So the final answer is: No\n",
      "\n",
      "Question: {input}\n",
      "Are followup questions needed here:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New approaches to read next:\n",
    "\n",
    "**Reflexion** ([Shinn & Labash 2023](https://arxiv.org/abs/2303.11366)) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills.\n",
    "\n",
    "**Chain of Hindsight** (CoH; [Liu et al. 2023](https://arxiv.org/abs/2302.02676)) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
